# 上线配置
## 数据库配置
spark.local=false
jdbc.url=jdbc:mysql://10.45.191.46:3306/xiaopeng2_bi?user=hadoop&password=hadoopmaster2016abt&useUnicode=true&characterEncoding=utf-8&autoReconnect=true&failOverReadOnly=false
jdbc.user=hadoop
jdbc.pwd=hadoopmaster2016abt
jdbc.xiaopeng2.url=jdbc:mysql://10.116.123.113:3306/xiaopeng2?user=hadoop&password=hadoopmaster2016abt&useUnicode=true&characterEncoding=utf-8&autoReconnect=true&failOverReadOnly=false
jdbc.xiaopeng2bihip.url=jdbc:mysql://10.24.167.208:3306/xiaopeng2_bi?user=hadoop&password=hadoopmaster2016abt&useUnicode=true&characterEncoding=utf-8&autoReconnect=true&failOverReadOnly=false
jdbc.xiaopeng2fx.url=jdbc:mysql://10.116.123.113:3306/xiaopeng2_faxing?user=hadoop&password=hadoopmaster2016abt&useUnicode=true&characterEncoding=utf-8&autoReconnect=true&failOverReadOnly=false
jdbc.driver=com.mysql.jdbc.Driver


# kafka 配置
kafka.metadata.broker.list=hadoopmaster:9092,hadoopslave1:9092,hadoopslave2:9092

kafka.topics.login=login
spark.checkpoint.login=/home/hduser/spark/spark-1.6.1/checkpointdir/login

kafka.topics.account=login,regi,order
spark.checkpoint.account=/home/hduser/spark/spark-1.6.1/checkpointdir/centurioncardaccount

kafka.topics.apppoints=order,points
spark.checkpoint.apppoints=/home/hduser/spark/spark-1.6.1/checkpointdir/apppoints

kafka.topics.basekpi=channel,active,order,regi,request,pubgame,pubgame
spark.checkpoint.basekpi=/home/hduser/spark/spark-1.6.1/checkpointdir/basekpi

kafka.topics.kpi=regi,order,login,active
spark.checkpoint.kpi=/home/hduser/spark/spark-1.6.1/checkpointdir/kpi_regi

kafka.topics.everylogin=login
spark.checkpoint.everylogin=/home/hduser/spark/spark-1.6.1/checkpointdir/appeverylogin

topicsApp=order,appdownload,login,appinstall
App2_2=file:///home/hduser/spark/spark-1.6.1/checkpointdir/App2_2

kafka.topics.click=channel,request,pubgame

kafka.topics.payclick=order,pubgame

#离线任务 数据目录配置
gamepublish.offline.regi=hdfs://hadoopmaster:9000/user/hive/warehouse/yyft.db/regi/*
gamepublish.offline.order=hdfs://hadoopmaster:9000/user/hive/warehouse/yyft.db/order/*
gamepublish.offline.pubgame=hdfs://hadoopmaster:9000/user/hive/warehouse/yyft.db/pubgame/*

fxdim.parquet=hdfs://hadoopmaster:9000/tmp/hive/fxdim.parquet

# spark 参数配置
coalesce.partitioin.num=40
spark.sql.shuffle.partitions=40
spark.memory.storageFraction=0.2

#redis相关配置
redis.host=10.46.133.54
redis.port=6379
redis.max.idle=50
redis.max.total=1000
redis.max.wait.millis=10000

#web交互的模式
web.url.mode=product
